{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f30e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from dataloader import read_mathqapython, MathQAPython "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397f5e9",
   "metadata": {},
   "source": [
    "# MathQA dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942a5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathQAPython(torch.utils.data.Dataset): \n",
    "    def __init__(self, instance_list, tokenizer, text_len, code_len): \n",
    "        self.data = instance_list \n",
    "        self.tokenizer = tokenizer \n",
    "        self.text_len = text_len\n",
    "        self.code_len = code_len\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        idx = idx + 2\n",
    "        instance = self.data[idx]\n",
    "        text = instance['text']\n",
    "        code = instance['code']\n",
    "        answer = instance['answer']\n",
    "\n",
    "        text_encode = self.tokenizer(text, max_length=self.text_len, \n",
    "                                     padding='max_length', return_tensors='pt')\n",
    "        code_encode = self.tokenizer(code, max_length=self.code_len, \n",
    "                                     padding='max_length', return_tensors='pt')\n",
    "        text_ids = text_encode['input_ids'].squeeze()\n",
    "        code_ids = code_encode['input_ids'].squeeze()\n",
    "\n",
    "        return text_ids.to(dtype=torch.long), code_ids.to(dtype=torch.long), answer\n",
    "\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c07aa",
   "metadata": {},
   "source": [
    "# Few-shot evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c297c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at some data\n",
    "data = read_mathqapython('data/mathqapython_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cccd785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '# mr . kramer , the losing candidate in a two - candidate election , received 942,568 votes , which was exactly 25 percent of all votes cast . approximately what percent of the remaining votes would he need to have received in order to have won at least 50 percent of all the votes cast ? n0 = 942568.0 n1 = 25.0 n2 = 50.0',\n",
       " 'code': 'n0 = 942568.0\\nn1 = 25.0\\nn2 = 50.0\\nt0 = n2 / 100.0\\nt1 = n1 / 100.0\\nt2 = t0 - t1\\nt3 = 1.0 - t1\\nt4 = t2 / t3\\nanswer = t4 * 100.0',\n",
       " 'dsl_code': 'divide(n2,const_100)|divide(n1,const_100)|subtract(#0,#1)|subtract(const_1,#1)|divide(#2,#3)|multiply(#4,const_100)|',\n",
       " 'reasoning': 'lets assume that candidate got 25 % votes and total votes is 100 . candidate won = 25 remaining = 75 to get 50 % , candidate requires 25 votes from 100 which is 25 % and 25 votes from 75 . 25 / 75 = 33.33 % which is approx 33 % . hence the answer is e',\n",
       " 'answer': 33.33333333333333,\n",
       " 'task_id': 35}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8ce2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d76e8e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1707\n",
      "# a multiple choice test consists of 4 questions , and each question has 5 answer choices . in how many r ways can the test be completed if every question is unanswered ? n0 = 4.0 n1 = 5.0\n",
      "n0 = 4.0\n",
      "n1 = 5.0\n",
      "\n",
      "answer = n1**min(n0, 5)\n",
      "\n",
      "# the hcf and lcm of two numbers m and n are respectively 6 and 210 . if m + n = 72 , then 1 / m + 1 / n is equal to n0 = 6.0 n1 = 210.0 n2 = 72.0 n3 = 1.0 n4 = 1.0\n",
      "n0 = 6.0\n",
      "n1 = 210.0\n",
      "n2 = 72.0\n",
      "n3 = 1.0\n",
      "n4 = 1.0\n",
      "t0 = n0 * n1\n",
      "answer = n2 / t0\n",
      "\n",
      "# in a kilometer race , a beats b by 48 meters or 12 seconds . what time does a take to complete the race ? n0 = 48.0 n1 = 12.0\n",
      "n0 = 48.0\n",
      "n1 = 12.0\n",
      "t0 = n0 / n1\n",
      "t1 = 1.0 * 1000.0\n",
      "t2 = t1 / t0\n",
      "answer = t2 - n1\n",
      "\n",
      "# in a school of 650 boys , 44 % of muslims , 28 % hindus , 10 % sikhs and the remaining of other communities . how many belonged to the other communities ? n0 = 650.0 n1 = 44.0 n2 = 28.0 n3 = 10.0\n",
      "n0 = 650.0\n",
      "n1 = 44.0\n",
      "n2 = 28.0\n",
      "n3 = 10.0\n",
      "t0 = n1 + n2\n",
      "t1 = n3 + t0\n",
      "t2 = 100.0 - t1\n",
      "t3 = n0 * t2\n",
      "answer = t3 / 100.0\n",
      "\n",
      "# a can do a piece of work in 4 hours ; b and c together can do it in 3 hours , while a and c together can do it 2 hours . how long will b alone take to do it ? n0 = 4.0 n1 = 3.0 n2 = 2.0\n",
      "n0 = 4.0\n",
      "n1 = 3.0\n",
      "n2 = 2.0\n",
      "t0 = 1.0 / n1\n",
      "t1 = 1.0 / n2\n",
      "t2 = 1.0 / n0\n",
      "t3 = t1 - t2\n",
      "t4 = t0 - t3\n",
      "answer = 1.0 / t4\n",
      "\n",
      "# in a group of 160 people , 90 have an age of more 30 years , and the others have an age of less than 20 years . if a person is selected at random from this group , what is the probability the person ' s age is less than 20 ? n0 = 160.0 n1 = 90.0 n2 = 30.0 n3 = 20.0 n4 = 20.0\n",
      "n0 = 160.0\n",
      "n1 = 90.0\n",
      "n2 = 30.0\n",
      "n3 = 20.0\n",
      "n4 = 20.0\n",
      "t0 = n0 - n1\n",
      "answer = t0 / n0\n",
      "\n",
      "# an art gallery has only paintings and sculptures . currently , 1 / 3 of the pieces of art are displayed , and 1 / 6 of the pieces on display are sculptures . if 1 / 3 of the pieces not on display are paintings , and 800 sculptures are not on display , how many pieces of art does the gallery have ? n0 = 1.0 n1 = 3.0 n2 = 1.0 n3 = 6.0 n4 = 1.0 n5 = 3.0 n6 = 800.0\n",
      "n0 = 1.0\n",
      "n1 = 3.0\n",
      "n2 = 1.0\n",
      "n3 = 6.0\n",
      "n4 = 1.0\n",
      "n5 = 3.0\n",
      "n6 = 800.0\n",
      "t0 = n0 / n1\n",
      "t1 = n0 - t0\n",
      "t2 = n6 / t1\n",
      "answer = t2 / t1\n",
      "\n",
      "# at what rate of compound interest per annum will a sum of rs . 1200 become rs . 1348.32 in 2 years ? n0 = 1200.0 n1 = 1348.32 n2 = 2.0\n",
      "import math\n",
      "n0 = 1200.0\n",
      "n1 = 1348.32\n",
      "n2 = 2.0\n",
      "t0 = n0 / 100.0\n",
      "t1 = n1 * 100.0\n",
      "t2 = n0 * 100.0\n",
      "t3 = t1 / t0\n",
      "t4 = t2 / t0\n",
      "t5 = math.sqrt(max(0, t3))\n",
      "t6 = math.sqrt(max(0, t4))\n",
      "answer = t5 - t6\n",
      "\n",
      "# in the first 10 overs of a cricket game , the run rate was only 6.2 . what should be the run rate in the remaining 40 overs to reach the target of 282 runs ? n0 = 10.0 n1 = 6.2 n2 = 40.0 n3 = 282.0\n",
      "n0 = 10.0\n",
      "n1 = 6.2\n",
      "n2 = 40.0\n",
      "n3 = 282.0\n",
      "t0 = n0 * n1\n",
      "t1 = n3 - t0\n",
      "answer = t1 / n2\n",
      "\n",
      "# during a car trip , maria stopped to rest after she traveled 1 / 2 of the total distance to her destination . she stopped again after she traveled 1 / 4 of the distance remaining between her first stop and her destination , and then she drove the remaining 135 miles to her detination . what was the total distance , in miles from maria ' s starting point to her destination ? n0 = 1.0 n1 = 2.0 n2 = 1.0 n3 = 4.0 n4 = 135.0\n",
      "n0 = 1.0\n",
      "n1 = 2.0\n",
      "n2 = 1.0\n",
      "n3 = 4.0\n",
      "n4 = 135.0\n",
      "t0 = n0 / n1\n",
      "t1 = n0 / n3\n",
      "t2 = t0 * t1\n",
      "t3 = t0 + t2\n",
      "t4 = n0 - t3\n",
      "answer = n4 / t4\n",
      "\n",
      "# a and b go around a circular track of length 600 m on a cycle at speeds of 36 kmph and 54 kmph . after how much time will they meet for the first time at the starting point ? n0 = 600.0 n1 = 36.0 n2 = 54.0\n",
      "n0 = 600.0\n",
      "n1 = 36.0\n",
      "n2 = 54.0\n",
      "t0 = n2 * 0.2778\n",
      "t1 = n1 * 0.2778\n",
      "t2 = t0 - t1\n",
      "answer = n0 / t2\n",
      "\n",
      "# a certain car can travel 32 kilometers on a liter of fuel . if the fuel tank â€™ s contents decrease by 3.9 gallons over a period of 5.7 hours as the car moves at a constant speed , how fast is the car moving , in miles per hour ? ( 1 gallon = 3.8 liters ; 1 mile = 1.6 kilometers ) n0 = 32.0 n1 = 3.9 n2 = 5.7 n3 = 1.0 n4 = 3.8 n5 = 1.0 n6 = 1.6\n",
      "n0 = 32.0\n",
      "n1 = 3.9\n",
      "n2 = 5.7\n",
      "n3 = 1.0\n",
      "n4 = 3.8\n",
      "n5 = 1.0\n",
      "n6 = 1.6\n",
      "t0 = n0 * n4\n",
      "t1 = t0 / n6\n",
      "t2 = n1 * t1\n",
      "answer = t2 / n2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\\n\\n\".join([example['text'] + '\\n' + example['code'] for example in data[0:12] ]) + '\\n\\n'\n",
    "print(len(tokenizer(few_shot_prompt)['input_ids']))\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5a90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = MathQAPython(data, tokenizer, 256, 256)\n",
    "\n",
    "loader = DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63684c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc75139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for batch in loader: \n",
    "    ids, gt, gt_answer = batch\n",
    "    encoded_few_shot_prompt = tokenizer(few_shot_prompt, return_tensors=\"pt\")['input_ids']\n",
    "    few_shot_ids = torch.cat([encoded_few_shot_prompt, ids], axis=1)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=few_shot_ids, \n",
    "        do_sample=True,\n",
    "        temperature=0.4, \n",
    "        max_length=2048\n",
    "        )\n",
    "    print(\"completion\" + \"#\"*20)\n",
    "    print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "    print(\"prompt\" + \"#\"*20)\n",
    "    print(tokenizer.decode(ids.squeeze(), skip_special_tokens=True))\n",
    "    print('#'*20 + 'ground truth code')\n",
    "    print(tokenizer.decode(gt.squeeze(), skip_special_tokens=True))\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6599cf4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'span'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26589/501420091.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_few_shot_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'answer.*?\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'span'"
     ]
    }
   ],
   "source": [
    "start_idx = encoded_few_shot_prompt.size()[1]\n",
    "completion = tokenizer.decode(generated_ids[0, start_idx:])\n",
    "program = completion[:re.search('answer.*?\\n', completion).span()[1]]\n",
    "print(program)\n",
    "loc={}\n",
    "a = exec(code+last_line, globals(), loc)\n",
    "print(loc['answer'])\n",
    "print('ground truth answer: ', gt_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fdce2",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bcec5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e061ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at /home/zhangir/.cache/huggingface/transformers/08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at /home/zhangir/.cache/huggingface/transformers/12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at /home/zhangir/.cache/huggingface/transformers/6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at /home/zhangir/.cache/huggingface/transformers/3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at /home/zhangir/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = read_mathqapython('data/mathqapython_train.json')\n",
    "val_data = read_mathqapython('data/mathqapython_dev.json')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_set = MathQAPython(data[0:10], tokenizer, 1024, 1024)\n",
    "dev_set = MathQAPython(data[0:2], tokenizer, 1024, 1024)\n",
    "# loader = DataLoader(train_set, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d327288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at /home/zhangir/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin from cache at /home/zhangir/.cache/huggingface/transformers/b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2cbbb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='./results', \n",
    "                                  num_train_epochs=2, \n",
    "                                  logging_steps=2, \n",
    "                                  save_steps=2, \n",
    "                                  per_device_train_batch_size=2, \n",
    "                                  per_device_eval_batch_size=2, \n",
    "                                  weight_decay=0.01, \n",
    "                                  logging_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32a6c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(data):\n",
    "    return {'input_ids': torch.stack([f[0] for f in data]),\n",
    "            'labels': torch.stack([f[1] for f in data]) \n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1d610e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 01:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.439100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-2\n",
      "Configuration saved in ./results/checkpoint-2/config.json\n",
      "Model weights saved in ./results/checkpoint-2/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-4\n",
      "Configuration saved in ./results/checkpoint-4/config.json\n",
      "Model weights saved in ./results/checkpoint-4/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-6\n",
      "Configuration saved in ./results/checkpoint-6/config.json\n",
      "Model weights saved in ./results/checkpoint-6/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-8\n",
      "Configuration saved in ./results/checkpoint-8/config.json\n",
      "Model weights saved in ./results/checkpoint-8/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8, training_loss=2.798557788133621, metrics={'train_runtime': 103.6475, 'train_samples_per_second': 0.154, 'train_steps_per_second': 0.077, 'total_flos': 8358627115008.0, 'train_loss': 2.798557788133621, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer(model=model, args=training_args, train_dataset=train_set, \n",
    "        eval_dataset=dev_set, data_collator=data_collator).train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
